{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Data cleaning and pre-processing\n\nData cleansing and preparation is the most critical first step in any data science project. Evidence shows that data scientists will up to 70% of their time cleaning data.\n\nThis notebook walks you through the initial steps of data cleansing and pre-processing in Python.\n\nThe dataset that we will use for this activity has already been downloaded and is available to the notebook as ```CGD - COVID education policy tracking.csv```. The dataset was downloaded from the [Centre for Global Development](https://www.cgdev.org/blog/schools-out-now-what).\n\nLet's assume that we are going to create a bar chart showing the \n\nThis is a simplistic use of the data set, however the principles are the same as you will use for your coursework.\n\n\n## Step 1: Load the data set and remove unnecessary columns\n\nThe first step is to import the Python libraries that you will use for pre-processing the data. The most popular Python libraries for working with data are Numpy, Matplotlib and Pandas. You may have used Numpy and Pandas in COMP00015 Intro to programming in Python.\n\nNumpy is mathematical library. \n\nPandas is used for importing and managing data sets.\n\nMatplotlib is used for making charts.\n\nYou can import these libraries using a shortcut alias. Run the following cell."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Load the data into pandas\n\n\nWe need to load the ```CGD - COVID education policy tracking.csv``` file into a pandas dataframe. This will allow us to explore and perform some basic cleanup tasks, removing unwanted information, which can slow down the data processing.\n\nThe first tasks is to load the data and remove (delete) the first line. This contain contains irrelevant information, a logo, instead of column headings which would prevents pandas from parsing the data set.\n\nOnce you have data in Python, youâ€™ll want to see the data has loaded, and confirm that the expected columns and rows are present and if there are missing values anywhere. To do this you can simply print the data in the Jupyter notebook by typing the name of the dataframe. This will result in nicely formatted output however by default only around 20 columns and 60 rows are displayed. The additional lines below are set to allow you to view all the columns, not not all rows, of the data.\n\nRun the following cell. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load the csv file into a pandas dataframe and skip the first line which contains the logo\ndf = pd.read_csv('CGD - COVID education policy tracking.csv', skiprows = 1, low_memory = False)\n\n# View the contents of the dataframe\npd.show_versions()\npd.display.options.width\npd.display.options.width = 69\ndf",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.6.6.final.0\npython-bits: 64\nOS: Linux\nOS-release: 4.14.29-linuxkit\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\nLOCALE: en_US.UTF-8\n\npandas: 0.23.4\npytest: 3.8.2\npip: 19.3.1\nsetuptools: 41.6.0\nCython: 0.29.14\nnumpy: 1.16.2\nscipy: 1.1.0\npyarrow: 0.13.0\nxarray: None\nIPython: 7.1.1\nsphinx: 1.8.1\npatsy: 0.5.1\ndateutil: 2.8.1\npytz: 2019.3\nblosc: None\nbottleneck: 1.2.1\ntables: 3.4.4\nnumexpr: 2.6.8\nfeather: None\nmatplotlib: 3.0.0\nopenpyxl: 2.5.8\nxlrd: 1.1.0\nxlwt: 1.3.0\nxlsxwriter: 1.1.1\nlxml: 4.2.5\nbs4: 4.6.3\nhtml5lib: 0.9999999\nsqlalchemy: 1.2.14\npymysql: 0.9.2\npsycopg2: 2.7.5 (dt dec pq3 ext lo64)\njinja2: 2.10\ns3fs: None\nfastparquet: None\npandas_gbq: None\npandas_datareader: None\n",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'pandas' has no attribute 'display'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-bd0d51f7d201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# View the contents of the dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_versions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m69\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'display'"
          ]
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Delete unnecessary columns with text descriptions\n\nDelete columns that contain text descriptions that you don't need e.g. urls, text.\n\nThe pandas dataframe [drop()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) method can be used to remove column. \n\nThe following cell shows examples of some of the ways you can remove columns. Use the documentation link above for other options.\n\nRun the following cell to the remove the indicated columns from the .csv.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Remove a single column with the name 'Source for Re-opening' \ndf = df.drop(['Source for Re-opening'], axis = 1)\n\n# Remove two columns named 'Facebook Page' and 'Official COVID Education Policy Document'\ndf.drop(['Facebook Page', 'Official COVID Education Policy Document'], axis = 1)\n\n# Remove columns AO (40) and AP (41)\ndf.drop(df.columns[[40, 41]], axis = 1, inplace = True)\n\n# View the data again to check the final 5 columns have been removed\ndf",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Look at the csv and remove at least one other unnecessary column. \n\nUse the cell below to add your code and run the cell when you are ready."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Add code to remove another unnecessary column\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Delete all columns that contain only one value, or have more than 50% of the values missing\n\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = df.dropna(thresh = half_count, axis = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Rename the data set\n\nIt is a good practice to name the filtered data set differently to separate it from the original data. This ensures that you still have the raw data in case you need to return it."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Save the dataframe back to a new csv\ndf.to_csv(\"cleaned_dataset.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Step 2: Explore the data\n\n### Understand the data\n\nNow that you have set up the data, you should still take some time to explore it and understand what each column represents. This manual review of the data set is important to avoid errors in the data analysis and modeling process.\n\nTo simplify the process, you can create a DataFrame using the columns in the data dictionary, the data type, the first row value, and the name of the description.\n\nAs you explore these features, you can focus on any of the following columns:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Determine the target column\n\nBy exploring the filtered data set, you need to create a dependent variable matrix and an independent variable vector. First, you should determine the appropriate column to use as the modeling target column based on the question you are answering. For example, if you want to predict the development of cancer, or the opportunity for a letter of credit to be approved, you need to find a column with a disease status or a loan grant ad to use it as the target column.\n\nFor example, if the target column is the last column, you can create a dependent variable matrix by typing:\n\nX = dataset.iloc [:,: - 1] .values\n\nThis first colon (:) means that we want all the lines in our data set.:-1Indicates that we want to get all the data columns except the last one. At the end of last month, we want all the values.. values\n\nTo get an argument vector that contains only the last column of data, type\n\ny = dataset.iloc [:, - 1] .values\n\nStep 3. Prepare machine learning function\n\nFinally, it is time to prepare to provide the functionality of the ML algorithm. To clean up the dataset, you needHandling missing values â€‹â€‹and classification featuresBecause the mathematical assumptions of most machine learning models are numerical and do not contain missing values. Also, if you try to train models such as linear regression and logistic regression using data with missing or non-numeric values, thenScikit-learnThe library will return an error.\nHandling missing values\n\nLost data can be the most common feature of unclean data. These values â€‹â€‹are usually in the form of NaN or None.\n\nHere are a few reasons for missing values: sometimes missing values â€‹â€‹because they don't exist, or because of improper data collection or improper data entry. For example, if someone is underage and the issue applies to someone older than 18, the question will contain missing values. In this case, the value of the question is incorrect.\n\nThere are several ways to fill in missing values:\n\n    If your data set is large enough and the percentage of missing values â€‹â€‹is high (for example, more than 50%), you can delete the rows that contain the data;\n    You can use 0 to fill all empty variables to handle values;\n    you can useScikit-learnIn the libraryImputerClass fills missing values â€‹â€‹with data (mean, median, most_frequent)\n    You can also decide to fill in the missing values â€‹â€‹directly with any value in the same column.\n\nThese decisions depend on the type of data, the actions you want to perform on the data, and the reasons for the missing values. In fact, just because something is popular doesn't necessarily make it the right choice. The most common strategy is to use an average, but depending on your data, you might take a completely different approach.\nProcessing classified data\n\nMachine learning uses only numeric values â€‹â€‹(float or int data types). However, datasets typically contain object data types that are to be converted to numbers. In most cases, the categorical values â€‹â€‹are discrete and can be encoded as dummy variables, assigning a number to each category. The easiest way is to use One Hot Encoder, specifying the index of the column to be processed:\n\nFrom sklearn.preprocessing import OneHotEncoder onehotencoder = OneHotEncoder(categorical_features = [0])X = onehotencoder.fit_transform(X).toarray()\n\nHandling inconsistent data entry\n\nFor example, when there are different unique values â€‹â€‹in the column, an inconsistency can occur. You can consider using different capitalization methods, simple error imprints, and inconsistent formats to form an idea. One way to remove data inconsistencies is to remove spaces before or after the entry name and convert everything to lowercase.\n\nHowever, if there are a large number of inconsistent unique entries, you cannot manually check the closest match. you can use itFuzzy WuzzyThe package identifies which strings are most likely to be the same. It accepts two strings and returns a ratio. The closer the ratio is to 100, the more likely you are to unify the string.\nProcessing date and time\n\nThe specific type of data inconsistency is inconsistent in date format, such as dd / mm / yy and mm / dd / yy in the same column. Your date value may not be the correct data type, which will not allow you to perform actions efficiently and gain insight from it. This time you can usedatetimePackage to fix the type of date.\nScaling and normalization\n\nIf you need to specify another change in the number of changes that is not equal to the other, scaling is important. With scaling, you can ensure that they are not used as the primary predictor just because they are very large. For example, if you use a person's age and salary in a forecast, some algorithms will pay more attention to salary because it is bigger, which makes no sense.\n\nNormalization involves converting or converting a data set to a normal distribution. imageSVMSuch algorithms converge much faster on standardized data, so it makes sense to standardize the data to get better results.\n\nThere are many ways to perform feature scaling. In short, we put all the features in the same scale so that no one function is dominated by another. For example, you can use the sklearn.preprocessing packageStandardScalerClass to fit and transform the data set:\n\nFrom sklearn.preprocessing import StandardScaler sc_X = StandardScaler()X_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)Since you don't need to put it in the test set, you only need to apply the conversion.Sc_y = StandardScaler()\nY_train = sc_y.fit_transform(y_train)\n\nSave as CSV\n\nTo ensure that you still have raw data, it's a good idea to store the final output of each part or stage of the workflow in a separate csv file. This way, you can make changes in the data processing flow without having to recalculate everything.\n\nAs we have done before, you can use pandas To_csv()The function stores the DataFrame as a .csv .\n\nMy_dataset.to_csv(\"processed_data/cleaned_dataset.csv\", index=False)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}